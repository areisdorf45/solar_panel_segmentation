{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4d7440-7721-4dda-b0c1-8853668b2b21",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d08a505d-e185-4447-abb8-9d2fba6bba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, layers\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, BatchNormalization, MaxPooling2D, Conv2DTranspose, concatenate, Activation, Concatenate\n",
    "from tensorflow.keras.metrics import IoU, BinaryIoU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87808c-45fb-4456-8d98-ea69ce4874e5",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4a6e3f6-fbc9-4f10-871f-2901d39e8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.environ['HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "561d085a-f136-4e27-b427-4460e158d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_small_X = os.path.join(home,'raw_data/small_dataset/sample_images')\n",
    "path_small_y = os.path.join(home,'raw_data/small_dataset/sample_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1da0666-312e-4a2c-a6c7-91ee047f73d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d000d156-83bc-4b49-981f-a9d5c0416363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split (path_X, path_y, split_ratio):\n",
    "    X_names = os.listdir(path_X)\n",
    "    y_names = os.listdir(path_y)\n",
    "    y_path = [f'{path_y}/{file}' for file in y_names]\n",
    "    X_path = [f'{path_X}/{file}' for file in X_names]\n",
    "    train_X, val_X = X_path[:int(len(X_path)*split_ratio)], X_path[int(len(X_path)*split_ratio):]\n",
    "    train_y, val_y = y_path[:int(len(y_path)*split_ratio)], y_path[int(len(y_path)*split_ratio):]\n",
    "    return train_X, val_X, train_y, val_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16dde256-6fc2-49cd-ba13-553180cbd3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_val_split (path_small_X, path_small_y, split_ratio) #small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9aeddb1e-9902-49bb-83ec-4df58ad75f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_matching_input_labels(X_names, y_names):\n",
    "    for x, y in zip(X_names, y_names):\n",
    "        if os.path.basename(x) != os.path.basename(y):\n",
    "            raise ValueError(f\"X and Y not matching: {x, y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1faf57c2-9e80-49b6-9436-3c8974bae7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_matching_input_labels(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a8c04b9-c886-4898-ab22-d994e2f64593",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_matching_input_labels(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed8f340c-d603-4f01-8156-30ef64535ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(image_path, mask_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    image = tf.image.decode_png(image, channels = 3)\n",
    "    mask = tf.image.decode_png(mask, channels = 1) / 255 \n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27bd5188-fe0a-4080-842a-0776ec69d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data (X_path, y_path, batch_size):\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_path, y_path))\n",
    "    return ds_train.map(process_path).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8a4e9-99d8-4820-91f1-4d04524ae627",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dccd6a75-528c-45a1-8087-4a857e436584",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = batch_data(train_X, train_y, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d8bf7-8b05-4618-86b7-c96eccefdf74",
   "metadata": {},
   "source": [
    "### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f4b3ee9-b134-407b-99de-59d59aa9a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = batch_data(val_X, val_y, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401aa3a9-a713-49f9-af9d-ba8b55679f42",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f57f275-a169-44c1-a577-49c135c93ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_X_TEST = os.path.join(home,'raw_data/TEST_slices/test_image_slices')\n",
    "path_y_TEST = os.path.join(home,'raw_data/TEST_slices/test_mask_slices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1264cea-bf4a-4570-a62b-cd67f54a34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_test (path_X, path_y, batch_size):\n",
    "    X_names = os.listdir(path_X)\n",
    "    X_path = [f'{path_X}/{file}' for file in X_names]\n",
    "    y_names = os.listdir(path_y)\n",
    "    y_path = [f'{path_y}/{file}' for file in y_names]\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_path, y_path))\n",
    "    return ds_train.map(process_path).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a08703db-88a9-47e4-a553-83ac4ea9b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_dataset = batch_data_test(path_X_TEST, path_y_TEST, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea696bf-c7b4-4e49-89ba-2d03c8669ced",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f50a6e10-5c32-449a-8231-9c62defaa603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters, (3,3), padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4ecf07b-46e6-4eb8-b81a-6d86db51c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(inputs, num_filters):\n",
    "    x = conv_block(inputs, num_filters) #can be used as skip connection \n",
    "    p = MaxPooling2D((2,2))(x)\n",
    "    return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "001fd9da-03a9-4ca4-af47-9b239d406f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(inputs, skip_features, num_filters): #skip features are going to be the x returned from the encoder block\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f7e53c2-e63a-4bcf-b6d1-a80bd02f0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(targets, inputs, smooth=1e-6):\n",
    "    \n",
    "    #flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    intersection = K.sum(targets * inputs)\n",
    "    dice = (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n",
    "    return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8ef0801-13bb-41b2-848c-3fd856183794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(img_height, img_width, channels):\n",
    "    \n",
    "    inputs = Input((img_height, img_width, channels))\n",
    "    inputs = Lambda(lambda x: x / 255)(inputs) #Normalize the pixels by dividing by 255\n",
    "\n",
    "    #Encoder - downscaling (creating features/filter)\n",
    "    skip1, pool1 = encoder_block(inputs, 16)\n",
    "    skip2, pool2 = encoder_block(pool1, 32) \n",
    "    skip3, pool3 = encoder_block(pool2, 64)\n",
    "    skip4, pool4 = encoder_block(pool3, 128) \n",
    "    \n",
    "    #Bottleneck or bridge between encoder and decoder\n",
    "    b1 = conv_block(pool4, 256)\n",
    "    \n",
    "    #Decoder - upscaling (reconstructing the image and giving it precise spatial location)\n",
    "    decoder1 = decoder_block(b1, skip4, 128)\n",
    "    decoder2 = decoder_block(decoder1, skip3, 64)\n",
    "    decoder3 = decoder_block(decoder2, skip2, 32)\n",
    "    decoder4 = decoder_block(decoder3, skip1, 16)\n",
    "    \n",
    "    #Output\n",
    "    outputs = Conv2D(1, (1, 1), padding='same', activation='sigmoid')(decoder4)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    iou = BinaryIoU()\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=dice_loss, metrics=['accuracy', iou])\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d0b5984-3721-44ca-8a08-ca8c95fe17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet(256, 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d25d69a8-86cd-4b1a-a3b9-526c9b770360",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '../tmp/checkpoint'\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True, monitor='val_loss', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "787df192-b4f3-46f6-b1c4-73c7fc9759cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "472/472 [==============================] - 78s 161ms/step - loss: 0.3755 - accuracy: 0.8698 - binary_io_u_4: 0.6691 - val_loss: 0.3644 - val_accuracy: 0.8619 - val_binary_io_u_4: 0.6605\n",
      "Epoch 2/500\n",
      "472/472 [==============================] - 76s 160ms/step - loss: 0.2876 - accuracy: 0.9040 - binary_io_u_4: 0.7259 - val_loss: 0.2924 - val_accuracy: 0.8968 - val_binary_io_u_4: 0.7186\n",
      "Epoch 3/500\n",
      "472/472 [==============================] - 76s 160ms/step - loss: 0.2581 - accuracy: 0.9142 - binary_io_u_4: 0.7484 - val_loss: 0.2947 - val_accuracy: 0.8990 - val_binary_io_u_4: 0.7164\n",
      "Epoch 4/500\n",
      "472/472 [==============================] - 76s 160ms/step - loss: 0.2393 - accuracy: 0.9206 - binary_io_u_4: 0.7636 - val_loss: 0.2528 - val_accuracy: 0.9290 - val_binary_io_u_4: 0.7619\n",
      "Epoch 5/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.2251 - accuracy: 0.9257 - binary_io_u_4: 0.7757 - val_loss: 0.2311 - val_accuracy: 0.9270 - val_binary_io_u_4: 0.7737\n",
      "Epoch 6/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.2157 - accuracy: 0.9288 - binary_io_u_4: 0.7834 - val_loss: 0.2231 - val_accuracy: 0.9287 - val_binary_io_u_4: 0.7801\n",
      "Epoch 7/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.2007 - accuracy: 0.9337 - binary_io_u_4: 0.7962 - val_loss: 0.2005 - val_accuracy: 0.9388 - val_binary_io_u_4: 0.8010\n",
      "Epoch 8/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.1918 - accuracy: 0.9367 - binary_io_u_4: 0.8040 - val_loss: 0.2521 - val_accuracy: 0.9101 - val_binary_io_u_4: 0.7498\n",
      "Epoch 9/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.1863 - accuracy: 0.9385 - binary_io_u_4: 0.8088 - val_loss: 0.2609 - val_accuracy: 0.9236 - val_binary_io_u_4: 0.7523\n",
      "Epoch 10/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.1755 - accuracy: 0.9419 - binary_io_u_4: 0.8183 - val_loss: 0.2061 - val_accuracy: 0.9340 - val_binary_io_u_4: 0.7957\n",
      "Epoch 11/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.1699 - accuracy: 0.9437 - binary_io_u_4: 0.8234 - val_loss: 0.2073 - val_accuracy: 0.9408 - val_binary_io_u_4: 0.7963\n",
      "Epoch 12/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.1638 - accuracy: 0.9456 - binary_io_u_4: 0.8287 - val_loss: 0.1749 - val_accuracy: 0.9457 - val_binary_io_u_4: 0.8230\n",
      "Epoch 13/500\n",
      "472/472 [==============================] - 76s 160ms/step - loss: 0.1596 - accuracy: 0.9470 - binary_io_u_4: 0.8326 - val_loss: 0.1997 - val_accuracy: 0.9360 - val_binary_io_u_4: 0.8007\n",
      "Epoch 14/500\n",
      "472/472 [==============================] - 76s 161ms/step - loss: 0.1571 - accuracy: 0.9479 - binary_io_u_4: 0.8350 - val_loss: 0.1778 - val_accuracy: 0.9463 - val_binary_io_u_4: 0.8214\n",
      "Epoch 15/500\n",
      "472/472 [==============================] - 76s 160ms/step - loss: 0.1537 - accuracy: 0.9490 - binary_io_u_4: 0.8382 - val_loss: 0.1772 - val_accuracy: 0.9458 - val_binary_io_u_4: 0.8211\n",
      "Epoch 16/500\n",
      "472/472 [==============================] - 76s 160ms/step - loss: 0.1508 - accuracy: 0.9499 - binary_io_u_4: 0.8407 - val_loss: 0.1837 - val_accuracy: 0.9463 - val_binary_io_u_4: 0.8168\n",
      "Epoch 17/500\n",
      "472/472 [==============================] - 76s 160ms/step - loss: 0.1467 - accuracy: 0.9512 - binary_io_u_4: 0.8445 - val_loss: 0.2071 - val_accuracy: 0.9320 - val_binary_io_u_4: 0.7931\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs = 500, callbacks=[es, checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f2e20b3-014f-472c-acee-1e9efe8eb832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565/565 [==============================] - 27s 47ms/step - loss: 0.2121 - accuracy: 0.9379 - binary_io_u_4: 0.7929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.212067112326622, 0.9378632307052612, 0.7928847670555115]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(TEST_dataset)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
